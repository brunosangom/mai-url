Hello, my name is Bruno Sánchez Gómez, and this is my presentation for coursework number 2 of URL. Today I will be talking about the paper "StackGAN++ Realistic Image Synthesis with Stack Generative Adversarial Networks".

This is a brief overview of what we will see today. I will start with an introduction where I talk about the problem addressed in the paper, and then I will go into detail about how it is addressed. Later, I will go over the results reported in the paper, both quantitative and qualitative, as well as its limitations and an ablation study. Finally, I will summarize everything in a conclusion and give some of my personal thoughts.

So, let's first go over what the paper is about.

The main area of research for the paper is GANs for image generation. Specifically StackGAN++ focuses on trying to generate high-resolution images (of 256x256 pixels) for both unconditional image generation as well as text-to-image synthesis. However, GANs are infamous for some of its limitations, such as training instability, which is caused by its sensibility to hyperparameters, and mode collapse, which causes the generated data to have very low diversity. On top of these, there is an added difficulty in generating high-resolution images. This occurs because high-dimensional image spaces make it hard for model and data distribution to overlap, which leads to poor gradients and ineffective training.

These issues are addressed by StackGAN through its two main contributions, which are Conditioning Augmentation, a technique for improving sample diversity by augmenting the image and text pairs, and two newly proposed GAN frameworks: StackGANv-1, a two-stage GAN, and StackGAN-v2, a multi-stage GAN with a tree-like structure. The two of of them are designed for both unconditional and conditional image synthesis with high resolution.

Let's take a look at the methodology behind each of these contributions. First of all, we'll go over the Conditioning Augmentation technique.

Its main idea is to augment the text conditioning to improve sample diversity and stabilize GAN training. The motivation behind it is that the latent space for text embeddings is usually high-dimensional, around the hundreds of dimensions, which causes discountinuities in the latent data manifold when there is limited data available. This makes the whole system fragile against even small modifications to the text conditionings. How CA addresses this is by sampling new embeddings from a Gaussian distribution whose parameters are calculated by learnable functions of the original text embeddings. This way, we can create an arbitrary number of additional image-text pairs that will help make the conditioning manifold smoother. To further enforce this smoothness, a regularization term is also added to the loss. This term is simply the Kullback–Leiber divergence between the Gaussian distribution with the parameters given by the learned functions and a typical multivariate 0,1 Gaussian distribution.

Now that we know how the data is augmented, let's take a look at the two proposed GAN architectures. First, we have StackGAN-v1. Its core idea is to decompose text-to-image generation into a sketch refinement process. This is modeled through a two-stage GAN framework, where the essential difference with a typical GAN framework is that the output of the generator is used as input for a second one in replacement of random noise. Let's take a closer look. In this diagram we can see that the first stage generator is fed the augmented conditioning along with some random noise, from which it generates an initial 64x64 image (in typical GAN fashion). Then, the stage two generator also receives the augmented conditioning but, instead of noise, this time it is accompanied by the output from the previous stage, which is now used as a base sketch, from which the generator learns how to create the final 256x256 output images. The discriminators of StackGAN-v1 are trained in the usual GAN fashion, where they learn to distinguish between real and fake images, leveraging also the conditioning information. In the case of the Stage-I discriminator, downsampled versions of the 256x256 images are used for the real image set.

As for StackGAN-v2, its core idea is for it to be a more general end-to-end multistage framework with a tree-like structure. In this image we can see an example with three generators, although this number may be adjustable. As the diagram shows, in StackGAN-v2 all of the generators are integrated into one unified pipeline that simultaneously generates images at different scales. The main difference with StackGAN-v1 is that the image generators are jointly trained to approximate image distributions at multiple scales, instead of being split into two different stages with separate training cycles. This makes the predictions at different scales generally more consistent. For each of the scales of the generated images, we train a Joint Conditional Unconditional Discriminator (or JCU) that makes two predictions at once: The first regarding whether the image is real or fake, and the second whether the image matches the text condition or not. This allows the whole system to be more robust by including both an unconditional and a conditional loss, that help the model better capture the real data distribution. Although it is not shown in this slide, it is worth mentioning that a color consistency regularization term is also added to the loss. This term encourages images generated from the same input at different scales to have similar color statistics, and this turned out to be crucial for unconditional tasks.

Now that we know how StackGAN++ approached the problem of high-resolution image generation, let's take a look at some of its experimental results.

Before delving into the quantitative and qualitative results, let's quickly go over this summary of the experiment setup. A collection datasets and competing methods were picked for both conditional and unconditional image generation. The datasets were picked based on their size, popularity and generality; and the competing methods were picked because they were some of the most popular state-of-the-art approaches in the moment of publication of this paper. As for the evaluation metrics, all of the results are based on three different metrics: Inception Score, Frechet Inception Distance and Human Rank. The arrows represent whether a higher or a lower value indicate better results. The only one of these that requires an explanation is the Human Rank, since it is a metric specifically gathered during the development of this paper, based on user studies. A group of 30 people were gathered and given the same text descriptions and five different images generated from these descriptions by five different models. Then, the average Human Rank represents how highly the images generated by a given model were ranked on average by these users.

So, let us now jump to the results themselves. First, we have some quantitative results. In the first table, StackGAN-v1 is compared against GAN-INT-CLS and GAWWN on three different data sets for the conditioned image generation task. As we can see, StackGAN-v1 clearly outperforms the other two models across the board, except for some outlier cases where it is still competitive. In the second table, we find a comparison between StackGAN-v1 and StackGAN-v2 on all of the different datasets. Here we can see that the second version surpasses the first according to both FID and the Human Rank, in most of the cases. The Inception Score shows a closer competition between the two. However, the overall results still seem to indicate that StackGAN-v2 has superior performance.

In order to back up this conclusion, let's now take a look at the qualitative results. Here we can see the images generated by StackGAN-v1 and StackGAN-v2, as well as all the other competing methods for unconditional image generation on the LSUN Bedroom dataset. We observe that the quality of the images generated with StackGAN-v2 is much better than the rest, especially considering that it is generating images at a higher resolution, which is a more difficult task. In some of the other generations we can find strange artifacts, but StackGAN-v2 seems to be the most robust model of the group.

Following that, let's take a look at the qualitative results for text-to-image generation. Here, we compare the two competing models with StackGAN-v1 and v2. The results again show that StackGAN-v2 creates less artifacts than v1 and other competing methods. Additionally, the images are significantly less blurry and more realistic thanks to the increased image resolution.

However, the StackGAN models also have some limitations, so let's take a look at those.

First, it was observed that, like many other previous GAN approaches, StackGAN-v1 suffers from mode collapse. Here we can see a visualization where t-SNE was used to embed images based on pixel value similarity with images generated by StackGAN-v1 on the left, and others generated by StackGAN-v2 on the right. It is obvious that a big region of the samples generated by StackGAN-v1 are, if not identical, very similar, which shows it is suffering from mode collapse. On the other side, we can see that StackGAN-v2 does not suffer from this issue, and its generated samples are diverse enough.

However, as it is common with any GAN approach, there are still some failed cases for both models, where the objects in the generated images are not identifiable, or there are a vast amount of artifacts that make the images strange and unrealistic.

Overall, the StackGAN approach seemed to be successful. But, let's take a look at how successful each of its improvements individually were. Here we can see the results of the component analyses performed for both StackGAN-v1 and v2 in the CUB dataset. In both cases, the different configurations were evaluated based on the Inception Score. In the analysis corresponding to StackGAN-v1, four components were tested: One-Stage versus Two-Stage GAN frameworks, Output Image Resolution, The impact of CA on text embeddings, and The effect of injecting the text embeddings twice or only to the first stage. The general trend is clear. Two-stage GANs perform better than one-stage ones. The same can be said for higher resolutions against lower ones. The CA turned out to also be generally impactful in a positive way, especially when injected to both stages. As for StackGAN-v2, four different configurations were tested against the baseline framework: three of them modifying the architecture of the generator component, and one without the JCU discriminators. The results show the gradual increase in image resolution to be the single most impactful component, vastly outperforming all the different variations of generator architectures. JCU, although not as impactful, also turned out to have a positive effect in inception score performance over the typical GAN discrimnator.

Finally, let's wrap up by gathering all the main conclusions extracted throughout the paper.

We saw that the three main contributions made by the StackGAN++ were generally successful. First, Conditioning Augmentation seemed to significantly improve sample diversity and training stability. Second, StackGAN-v1 seemed to generally succeed in generating high-resolution images. However, third, StackGAN-v2 improved its robustness by jointly approximating multiscale image distributions on both conditional and unconditional distributions. Overall, quantitative and qualitative results demonstrate superior performance over prior state-of-the-art methods, and the ablation studies further validate the effectiveness of each of its components.

To finish this presentation, I wanted to give some personal comments on what I thought were the negative and positive aspects of this paper. I'll start by pointing out some things that I thought the paper was missing. First, I did not think there was any justification not to use JCU Discriminators for StackGAN-v1, when they turned out to be a significant improvement for StackGAN-v2, since the two-stage framework also allowed for them to be used. Second, I thought it was strange that the authors did not include StackGAN-v2 in the quantitative analysis against other state-of-the-art methods. Instead they only compared them with the first version, and then the second was individually compared to the first. Finally, as a personal opinion, I believe the improvement in image quality shown in the quantitative results was not very significant. However—and this is the first positive aspect I want to highlight from the paper—, I do believe that the quality was on a similar level as the previous approaches while generating images at higher resolutions, which is a more difficult task. Aside from this, I found that the general idea of progressive refinement as a way to tackle high-resolution image synthesis was well-motivated, intuitive to understand, and empirically validated. As a final disclaimer, I will mention that this paper was published in 2018 and since then there has been a lot of advancement in the field of GANs, like for instance StyleGAN, which was published one year after StackGAN++, and perhaps some of the negative aspects that I have mentioned here have already been addressed in posterior works.

So this is the end of my presentation. Here you can find the references to the papers I have previously mentioned. I hope everything has been clearly explained, and if not, I'm open for questions through my email. Thank you for listening, and goodbye!
