Hello, my name is Bruno Sánchez Gómez, and this is my presentation for coursework number 2 of the URL class. Today I will be discussing the paper "StackGAN++ Realistic Image Synthesis with Stacked Generative Adversarial Networks".

This is a brief overview of what I will go over in this video. I will start with an introduction where I talk about the problem addressed in the paper. Then, I will go into detail about how it is addressed. Later, I will go over the results reported in the paper, both quantitative and qualitative, as well as its limitations and an ablation study. Finally, I will summarize everything in a conclusion and give some of my personal thoughts.

So, let's first go over what the paper is about.

The main area of research for the paper is GANs for image generation. Specifically StackGAN++ focuses on trying to generate high-resolution images (of 256x256 pixels) for both unconditional image generation as well as text-to-image synthesis. However, GANs are infamous for some of its limitations, such as training instability, which is caused by its sensibility to hyperparameters, and mode collapse, which causes the generated data to have very low diversity. On top of these, there is an added difficulty in generating high-resolution images. This occurs because high-dimensional image spaces make it hard for model and data distribution to overlap, which leads to poor gradients and ineffective training.

These issues are addressed by StackGAN through its two main contributions, which are Conditioning Augmentation, a technique for improving sample diversity by augmenting the image and text pairs, and two newly proposed GAN frameworks: StackGANv-1, a two-stage GAN, and StackGAN-v2, a multi-stage GAN with a tree-like structure. The two of of them are designed for both unconditional and conditional image synthesis with high resolution.

Let's take a look at the methodology behind each of these contributions. 

First of all, we'll go over the Conditioning Augmentation technique. Its main idea is to augment the text conditioning to improve sample diversity and stabilize GAN training. The motivation behind it is that the latent space for text embeddings is usually high-dimensional, around the hundreds of dimensions, which causes discountinuities in the latent data manifold when only limited data is available. This makes the whole system fragile against even small modifications to the text conditionings. How CA addresses this is by sampling new embeddings from a Gaussian distribution whose parameters are calculated by learnable functions of the original text embeddings. This way, we can create an arbitrary number of additional image-text pairs that will help make the conditioning manifold smoother. To further enforce this smoothness, a regularization term is also added to the loss. This term is simply the Kullback–Leiber divergence between the Gaussian distribution with the parameters given by the learned functions and a typical multivariate 0,1 Gaussian distribution. Now that we know how the data is augmented, let's take a look at the two proposed GAN architectures.

First, we have StackGAN-v1. Its core idea is to decompose text-to-image generation into a sketch refinement process. This is modeled through a two-stage GAN framework, where the essential difference with a typical GAN framework is that the output of the generator is used as input for a second generator, in replacement of random noise. In this diagram we can see that the first stage generator is fed the augmented conditioning along with some random noise, from which it generates an initial low-resolution image (in typical GAN fashion). Then, the stage two generator also receives the augmented conditioning but, instead of noise, this time it is accompanied by the output from the previous stage, which is now used as a base sketch for the second generator to create the final high-resolution output images. The discriminators of StackGAN-v1 are trained in the usual GAN fashion, where they learn to distinguish between real and fake images. In the case of the Stage-I discriminator, downsampled versions of the high-resolution images are used for the real image set.

As for StackGAN-v2, its core idea is for it to be a more general end-to-end multistage framework with a tree-like structure. In this image we can see an example with three generators, although this number may be adjustable. As the diagram shows, in StackGAN-v2 all of the generators are integrated into one unified pipeline that simultaneously generates images at different scales. The main difference with v1 is that the image generators are jointly trained to approximate image distributions at multiple scales, instead of being split in two different stages with separate training cycles. This makes the predictions at different scales generally more consistent. For each of the scales of the generated images, we train a Joint Conditional-Unconditional Discriminator (or JCU) that makes two predictions at once: The first regarding whether the image is real or fake, and the second whether the image matches the text condition or not. This allows the whole system to be more robust by including both an unconditional and a conditional loss, which help the model capture the real data distribution. Although it is not shown in this slide, it is worth mentioning that a color consistency regularization term is also added to the loss. This term encourages images generated from the same input at different scales to have similar color statistics, and it turned out to be crucial for unconditional tasks.

Now that we know how StackGAN++ approached the problem of high-resolution image generation, let's take a look at some of its experimental results.

Before delving into the quantitative and qualitative results, let's quickly go over this summary of the experiment setup. A collection of datasets and competing methods were picked for both conditional and unconditional image generation. The datasets were picked based on their size, popularity and generality; and the competing methods were picked out of the most popular state-of-the-art approaches at the moment of publication of this paper. As for the evaluation metrics, Inception Score, Frechet Inception Distance and Human Rank are computed. The arrows represent whether a higher or a lower value indicate better results. The only one of these that requires an explanation is the Human Rank, since it is a metric specifically gathered during the development of this paper. It is based on a user study performed on a group of 30 people who were given the same text descriptions and five images generated by different models from these descriptions. Then, they were asked to rank the images. The average Human Rank represents how highly the images generated by a given model were ranked on average by these users.

So, let us now jump to the results themselves. First, we have some quantitative results. In the first table, StackGAN-v1 is compared against GAN-INT-CLS and GAWWN on three different datasets of the conditioned image generation task. As we can see, v1 clearly outperforms the other two models across the board, except for some outlier cases where it is still competitive. In the second table, we find a comparison between v1 and v2 on all of the different datasets. Here we can see that v2 surpasses, in most of the cases, v1 according to both FID and the Human Rank. The Inception Score shows a closer competition between the two. However, the overall results still seem to indicate that v2 has superior performance.

In order to back up this hypothesis, let's now take a look at the qualitative results. Here we can see the images generated by v1 and v2, as well as all the other competing methods for unconditional image generation on the Bedroom subset of LSUN. We observe that the quality of the images generated with v2 is much better than the rest, especially considering that it is generating images at a higher resolution, which is a more difficult task. In some of the other generations we can find strange artifacts, but v2 seems to be the most robust model of the group.

Following that, let's take a look at the qualitative results for text-to-image generation on the CUB dataset. Here, we compare the two competing models with StackGAN-v1 and v2. The results again show that v2 creates less artifacts than v1 and other competing methods. Additionally, the images are significantly less blurry and more realistic thanks to the increased image resolution.

However, the StackGAN models also have some limitations, so let's take a look at those.

First, it was observed that, like many other previous GAN approaches, v1 suffers from mode collapse. Here we can see a visualization where t-SNE was used to embed images based on pixel value similarity. On the left, we find images generated by v1, and on the right are those generated by v2. It is obvious that a big region of the samples generated by v1 are, if not identical, very similar, which shows it is suffering from mode collapse. On the other side, we can see that v2 does not suffer from this issue, and its generated samples are diverse enough.

However, as it is common with any GAN approach, there are still some failed cases for both models, where the objects in the generated images are not identifiable, or there are a vast amount of artifacts that make the images strange and unrealistic.

Overall, the StackGAN approach seemed to be successful. But, let's take a look at how successful each of its improvements individually were. Here we can see the results of the component analyses performed for both StackGAN-v1 and v2 in the CUB dataset. In both cases, the different configurations were evaluated based on the Inception Score.

In the analysis corresponding to v1, four components were tested: One-Stage versus Two-Stage GAN frameworks, Output Image Resolution, The impact of CA on text embeddings, and The effect of injecting the text embeddings twice or only to the first stage. The general trend is clear. Two-stage GANs perform better than one-stage ones. The same can be said for higher resolutions against lower ones. The CA turned out to also be generally impactful in a positive way, especially when injected to both stages.

As for v2, four different configurations were tested against the baseline framework: one without the JCU discriminators, and the other three modifying the architecture of the stacked generator component. The results show the gradual increase in image resolution inside the stacked generator to be the single most impactful component, with the baseline model vastly outperforming all the different variations of architecture. JCU, although not as impactful, also turned out to have a positive effect in Inception Score performance over the typical GAN discrimnator.

Finally, let's wrap up by gathering all the main conclusions extracted throughout the paper.

We saw that the three main contributions made by the StackGAN++ were generally successful. First, Conditioning Augmentation seemed to significantly improve sample diversity and training stability. Second, StackGAN-v1 seemed to generally succeed in generating high-resolution images. However, third, StackGAN-v2 improved its robustness by jointly approximating multiscale image distributions on both conditional and unconditional distributions. Overall, quantitative and qualitative results demonstrate superior performance over prior state-of-the-art methods, and the ablation studies further validate the effectiveness of each of its components.

To finish this presentation, I wanted to give some personal comments on what I thought were the negative and positive aspects of this paper. I'll start by pointing out some things that I thought the paper was missing: First, I did not think there was any justification not to use JCU Discriminators for StackGAN-v1, when they turned out to be a significant improvement for v2, and the original two-stage framework also could have allowed for them to be used. Second, I thought it was strange that the authors did not include StackGAN-v2 in the quantitative analysis against other state-of-the-art methods. Instead they only compared v1 with the competing methods, and then v2 was individually compared to v1. Finally, as a personal opinion, I believe the improvement in image quality shown in the quantitative results was not very significant. However (and this is the first positive aspect I want to highlight from the paper), I do believe that the quality was on a similar level as the previous approaches while generating images at higher resolutions. This would mean it is able to mantain the same level of quality while performing a more difficult task. Aside from this, I found that the general idea of progressive refinement as a way to tackle high-resolution image synthesis was well-motivated, intuitive to understand, and empirically validated. As a final disclaimer, I will mention that this paper was published in 2018 and since then there has been a lot of advancement in the field of GANs. For instance, StyleGAN was published only one year after StackGAN++. This means that perhaps some of the negative aspects that I have mentioned here have already been addressed in posterior works.

So, this is the end of my presentation. Here you can find the references to the papers I have previously mentioned. I hope I have explained everything and, if not, I'm open for questions through my email. Thank you for listening, and goodbye!
