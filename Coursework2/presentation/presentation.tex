\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{float}
\usepackage{fontawesome}
\usepackage{multicol}
\usepackage{listings}
\usepackage{array}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{bookmark}
\newcommand{\tabitem}{~~\llap{\textbullet}~~}

% Beamer theme settings
\usetheme{Madrid}
\usecolortheme{default}

\title{URL Coursework 2}
\subtitle{StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks~\cite{stackgan++}}
\author{Bruno Sánchez Gómez}
\date{\today}

\setbeamertemplate{footline}{%
    \leavevmode%
    \hbox{%
        \begin{beamercolorbox}[wd=0.2\paperwidth,ht=2.5ex,dp=1.125ex,center]{author in head/foot}%
            \hspace*{1mm}Bruno Sánchez Gómez
        \end{beamercolorbox}%
        \begin{beamercolorbox}[wd=0.7\paperwidth,ht=2.5ex,dp=1.125ex,center]{title in head/foot}%
            \insertshortsubtitle
        \end{beamercolorbox}%
        \begin{beamercolorbox}[wd=0.1\paperwidth,ht=2.5ex,dp=1.125ex,center]{date in head/foot}%
            \hspace*{-1mm}\insertframenumber{} / \inserttotalframenumber
        \end{beamercolorbox}%
    }%
    \vskip0pt%
}

% Define a command for placeholder images for clarity
\newcommand{\paperfigure}[3][width=\textwidth]{%
    \begin{figure}%
        \centering%
        \includegraphics[#1]{figures/#2.png}%
        \caption{#3 (Source:~\cite{stackgan++})}%
    \end{figure}%
}


\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\section{Introduction}
\begin{frame}{Table of Contents}
    \tableofcontents[currentsection]
\end{frame}

\begin{frame}{Introduction}
    \begin{block}{Area of Research of the Paper}
        \textbf{Generative Adversarial Networks} (GANs) for Realistic Image Synthesis
    \end{block}
    \begin{itemize}
        \item High-resolution images ($256 \times 256$ pixels)
        \item Two tasks:
        \begin{itemize}
            \item \textit{Unconditional Image Generation} % Synthesizing diverse images from random noise, learning the underlying data distribution.
            \item \textit{Text-to-Image Synthesis} (Conditional Image Generation) % Generating photo-realistic images from textual descriptions (e.g., "a red bird with a short beak").
        \end{itemize}
    \end{itemize}
    \vspace{0.5cm}
    \begin{figure}
        \centering
        % \paperfigure[width=0.8\textwidth]{fig0_stackgan_overview}{Image examples for unconditional and conditional image generation tasks.}
    \end{figure}
\end{frame}

\begin{frame}{Limitations of Prior Work}
    \begin{itemize}
        \item \textbf{GAN Training Instability:}
        \begin{itemize}
            \item Sensitive to hyperparameters
            \item Can suffer from non-convergence
        \end{itemize}
        \item \textbf{Mode Collapse:}
        \begin{itemize}
            \item Limited variety of generated samples
            \item Fail to capture full diversity of the training data
        \end{itemize}
        \item \textbf{Limited to low-resolution images:}
        \begin{itemize}
            \item Training GANs for high-resolution images is especially difficult and unstable.
            \item Low overlap between model and data distributions $\rightarrow$ Poor gradients % High-dimensional image spaces make it hard for model and data distributions to overlap, leading to poor gradients.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Contributions by StackGAN++}
    \begin{enumerate}
        \item \textbf{Conditioning Augmentation (CA):} Improve sample diversity by augmenting the image-text pairs.
        \vspace{1em}
        \item \textbf{StackGAN:} Two GAN frameworks for conditional and unconditional image synthesis with high resolution ($256 \times 256$):
        \vspace{0.5em}
        \begin{itemize}
            \item \textbf{StackGAN-v1:} Two-stage GAN
            \vspace{0.25em}
            \item \textbf{StackGAN-v2:} Multi-stage GAN with a tree-like structure
        \end{itemize}
    \end{enumerate}
\end{frame}

\section{StackGAN++ Methodology}
\begin{frame}{Table of Contents}
    \tableofcontents[currentsection]
\end{frame}

\begin{frame}{Conditioning Augmentation (CA)}
    \centering
    \begin{minipage}{0.8\textwidth}
        \begin{block}{Core Idea}
            \centering
            Augment the text conditioning to improve sample diversity and stabilize GAN training
        \end{block}
    \end{minipage}
    \vspace{1em}
    \begin{itemize}
        \item The latent space for text embeddings, $\phi_t$, is high-dimensional
        \item Limited data causes discontinuity in the latent data manifold
        \item CA samples a new embedding $\hat{c}$ from a Gaussian distribution:
        \[
        \hat{c} = N(\mu_\theta(\phi_t), \Sigma_\theta(\phi_t))
        \]
        \item To further enforce smoothness over the conditioning manifold and avoid overﬁtting, a regularization term is added to the loss:
        \[
        \mathcal{L}_{\text{KL}} = D_{KL}\big(N(\mu_\theta(\phi_t), \Sigma_\theta(\phi_t))\, ||\, N(0, I)\big)
        \]
    \end{itemize}  
\end{frame}

% The input to the second stage is a 64x64 image from the first stage, along with the text embedding (conditioning), instead of noise

\begin{frame}{StackGAN-v1}
    \centering
    \begin{minipage}{0.8\textwidth}
        \begin{block}{Core Idea}
            \centering
            Decompose text-to-image generation into a sketch-refinement process
        \end{block}
    \end{minipage}
    \paperfigure[width=0.9\textwidth]{stackgan_v1}{StackGAN-v1 Architecture}
\end{frame}

\begin{frame}{StackGAN-v2}
    \centering
    \begin{minipage}{0.8\textwidth}
        \begin{block}{Core Idea}
            \centering
            A more general, end-to-end multi-stage framework with a tree-like structure
        \end{block}
    \end{minipage}
    \paperfigure[width=0.9\textwidth]{stackgan_v2}{StackGAN-v2 Architecture and JCU Discriminators}
\end{frame}

% JCU (Joint Conditional/Unconditional) Discriminator: It predicts whether an image is real or fake, and whether the image matches the text condition or not
% Generators are jointly trained to approximate image distributions at multiple scales, instead of being split into two stages
% Color-Consistency Regularization: Encourages images generated at different scales from the same input to have similar color statistics (mean, covariance). Crucial for unconditional tasks

\section{Results}
\begin{frame}{Table of Contents}
    \tableofcontents[currentsection]
\end{frame}

\begin{frame}{Experimental Setup}
    % Datasets Table
    \centering
    \begin{tabular}{|p{0.15\textwidth}|p{0.2\textwidth}|p{0.55\textwidth}|}
        \hline
        \multirow{6}{0.15\textwidth}{\textbf{Datasets}} & \multirow{2}{0.2\textwidth}{\textit{Unconditional}} & LSUN (Bedroom, Church) \\
                                                        &                                                     & ImageNet (Dog, Cat subsets) \\\cline{2-3}
                                                        & \multirow{4}{0.2\textwidth}{\textit{Conditional}}   & \multirow{4}{0.55\textwidth}{CUB-200-2011 (Birds) \newline Oxford-102 (Flowers) \newline MS COCO (Challenging general scenes)} \\
                                                        &                                                     & \\
                                                        &                                                     & \\
                                                        &                                                     & \\
        \hline
    \end{tabular}
    \vspace{1em}

    % Evaluation Metrics Table
    \centering
    \begin{tabular}{|p{0.15\textwidth}|p{0.5\textwidth}|}
        \hline
        \multirow{3}{0.15\textwidth}{\textbf{Evaluation Metrics}} & \textit{Inception Score (IS)}             $\uparrow$ \\\cline{2-2}
                                                                  & \textit{Fréchet Inception Distance (FID)} $\downarrow$ \\\cline{2-2}
                                                                  & \textit{Human Rank (HR)}                  $\downarrow$ \\
        \hline
    \end{tabular}
    \vspace{1em}

    % Competing Methods Table
    \centering
    \begin{tabular}{|p{0.15\textwidth}|p{0.2\textwidth}|p{0.45\textwidth}|}
        \hline
        \multirow{4}{0.15\textwidth}{\textbf{Competing Methods}} & \multirow{2}{0.2\textwidth}{\textit{Unconditional}} & DCGAN, WGAN, EBGAN-PT, \\
                                                                 &                                                     & LSGAN, WGAN-GP \\\cline{2-3}
                                                                 & \multirow{2}{0.2\textwidth}{\textit{Conditional}}   & \multirow{2}{0.45\textwidth}{GAN-INT-CLS, GAWWN} \\
                                                                 &                                                     & \\
        \hline
    \end{tabular}
\end{frame}

% Inception Score (IS): Measures image quality and diversity.
% Fréchet Inception Distance (FID): Measures similarity between generated and real image distributions.
% Human Rank (HR): User studies:
    % 30 human users (who are not authors of the paper) are given the same text descriptions. 
    % For each text description, they are shown the images generated by the different methods.
    % The users are asked to rank the results (the generated images) from the different methods.

\begin{frame}{Quantitative Results}
    \paperfigure[width=\textwidth]{quantitative_results}{Tables of quantitative results}
\end{frame}

\begin{frame}{Qualitative Results: Unconditional Image Generation}
    \paperfigure[width=\textwidth]{qualitative_results_unconditional}{Comparison of generated samples from LSUN Bedroom}
\end{frame}

\begin{frame}{Qualitative Results: Text-to-Image}
    \paperfigure[width=0.9\textwidth]{qualitative_results_conditional}{Comparison of generated samples with text descriptions from CUB}
\end{frame}

\begin{frame}{Limitations: StackGAN-v1 Mode Collapse}
    \paperfigure[width=\textwidth]{mode_collapse}{StackGAN-v1 suffers from mode collapse}
\end{frame}

\begin{frame}{Limitations: Failure Cases}
    \paperfigure[width=\textwidth]{failure_cases}{Failure cases of both StackGAN-v1 and StackGAN-v2}
\end{frame}

\begin{frame}{Ablation Studies}
    \paperfigure[width=0.5\textwidth]{ablation_v1}{Component analysis of StackGAN-v1}
    \paperfigure[width=0.9\textwidth]{ablation_v2}{Component analysis of StackGAN-v2}
\end{frame}

% Both on the CUB dataset

\section{Conclusions}
\begin{frame}{Table of Contents}
    \tableofcontents[currentsection]
\end{frame}

\begin{frame}{Conclusions}
    \begin{itemize}
        \item \textbf{Stacked/Multi-Stage GANs are Highly Effective:}
            \begin{itemize}
                \item Decomposing high-resolution image synthesis into progressive, manageable sub-problems (low-to-high resolution) is a key strategy for success.
            \end{itemize}
        \item \textbf{StackGAN-v1 Advanced Text-to-Image Synthesis:}
            \begin{itemize}
                \item First to achieve 256x256 photo-realistic images from text, with Conditioning Augmentation (CA) improving stability and diversity.
            \end{itemize}
        \item \textbf{StackGAN-v2 Offers Generality, Stability, and Quality:}
            \begin{itemize}
                \item Its tree-like architecture, joint multi-distribution approximation, and color-consistency regularization lead to more stable training, reduced mode collapse, and often higher quality for both conditional and unconditional tasks.
            \end{itemize}
        \item \textbf{Significant Progress in Realistic Image Generation:}
            \begin{itemize}
                \item The paper demonstrates a substantial leap in GANs' capability to generate detailed, high-resolution images.
            \end{itemize}
        \item \textbf{Future Directions:} Despite progress, achieving perfect realism, coherence for all inputs, and efficient training for extremely complex scenarios remain open challenges.
    \end{itemize}
    \vspace{0.3cm}
    \begin{block}{Personal Comment}
        The StackGAN++ paper presents a compelling and intuitive approach to tackling the difficult problem of high-resolution image synthesis. The idea of progressive refinement is well-motivated and empirically validated. StackGAN-v2's generalization to unconditional tasks and its more robust training are particularly strong contributions, setting a new benchmark at the time. The detailed ablation studies thoroughly support the design choices.
    \end{block}
\end{frame}

\begin{frame}[allowframebreaks]{References}
    \nocite{*} % To include all references from the .bib file, even if not explicitly cited in text (though StackGAN++ is cited)
    \bibliographystyle{unsrt} 
    \bibliography{references}
\end{frame}

\end{document}